



Develop an script which on given keyword goes to google and crawls for top 3 pages

for all the top 3 pages it stores the url

it then goes to domain page and classify what kind of website it is 

Also find the how many pages are there in the website via sitemap checks
use this python package to understand the sitemap url 
https://advertools.readthedocs.io/en/master/advertools.sitemaps.html 
and save the data in json

https://data.similarweb.com/api/v1/data?domain={{Url}}
Use this to get the below  data from the json it will response
$.Category
$.SiteName
$.Description
$.TopCountryShares
$.Engagments
$.EstimatedMonthlyVisits
$.GlobalRank
$.CountryRank
$.CategoryRank
$.TrafficSources
$.TopKeywords

and save the data json


Bucket the website into
 easy to crawl : if the website data is accessible 
 medium difficult to crawl : if the website needs login
 hard to crawl : if the website contains captcha solver 



 Use jina to convert the url into markdown: https://r.jina.ai/



Finally 
Use Open LLM to structure data and then
use  pydantic to validate data , if the data is not correct retry with LLM to structure the data

Save the data in DB
